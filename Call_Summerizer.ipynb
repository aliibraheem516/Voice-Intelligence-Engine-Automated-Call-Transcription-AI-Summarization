{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Cell 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "nGUd7MzSrMHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X2cG9FCO9W-7"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2 :  Load Whisper ASR Model"
      ],
      "metadata": {
        "id": "AUfy4RCLxVrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNZRMKgI86IW"
      },
      "outputs": [],
      "source": [
        "# Initialize the ASR pipeline with the Whisper model\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3 : Transcribe Audio to Text (test.wav ‚Üí transcription.txt)"
      ],
      "metadata": {
        "id": "-0t3IbdHxcnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Jl9ZEE9V5T"
      },
      "outputs": [],
      "source": [
        "# Process the audio file and get the transcription\n",
        "result = pipe(\"test.wav\", return_timestamps=True)\n",
        "\n",
        "# Extract the text from the result\n",
        "text = result['text']\n",
        "\n",
        "# Save the text to a file\n",
        "with open(\"transcription.txt\", \"w\") as file:\n",
        "    file.write(text)\n",
        "\n",
        "print(\"Transcription saved as transcription.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4 : Define Chunked Summarization Function"
      ],
      "metadata": {
        "id": "FWxGqizC5g8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_large_transcript(input_file, output_file=\"call_summary.txt\"):\n",
        "    \"\"\"Ultra-robust large transcript summarization with complete error handling\"\"\"\n",
        "    try:\n",
        "        # 1. MODEL SETUP\n",
        "        model_name = \"facebook/bart-large-cnn\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        summarizer = pipeline(\n",
        "            task=\"summarization\",\n",
        "            model=model_name,\n",
        "            tokenizer=tokenizer,\n",
        "            device=0 if torch.cuda.is_available() else -1,\n",
        "            framework=\"pt\"\n",
        "        )\n",
        "\n",
        "        # 2. INPUT HANDLING\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            transcript = f.read().strip()\n",
        "\n",
        "        if not transcript:\n",
        "            raise ValueError(\"Empty transcript file\")\n",
        "\n",
        "        # 3. SMART CHUNKING SYSTEM\n",
        "        def create_chunks(text, max_tokens=768, overlap=64):\n",
        "            \"\"\"Create chunks with token count and context overlap\"\"\"\n",
        "            tokens = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "            chunks = []\n",
        "            for i in range(0, len(tokens), max_tokens - overlap):\n",
        "                chunk_tokens = tokens[i:i + max_tokens]\n",
        "                chunks.append(tokenizer.decode(chunk_tokens, skip_special_tokens=True))\n",
        "            return chunks\n",
        "\n",
        "        chunks = create_chunks(transcript)\n",
        "        if not chunks:\n",
        "            raise ValueError(\"Failed to create text chunks\")\n",
        "\n",
        "        # 4. CHUNK PROCESSING WITH AUTO-ADJUSTMENT\n",
        "        successful_summaries = []\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            try:\n",
        "                chunk_word_count = len(chunk.split())\n",
        "\n",
        "                # Dynamic length calculation\n",
        "                max_len = min(128, max(30, chunk_word_count // 3))\n",
        "                min_len = max(15, max_len // 2)\n",
        "\n",
        "                # Safe summary generation\n",
        "                summary = summarizer(\n",
        "                    chunk,\n",
        "                    max_length=max_len,\n",
        "                    min_length=min_len,\n",
        "                    do_sample=False,\n",
        "                    truncation=True,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "\n",
        "                if summary and isinstance(summary, list) and 'summary_text' in summary[0]:\n",
        "                    successful_summaries.append(summary[0]['summary_text'])\n",
        "                    print(f\"‚úì Chunk {i}/{len(chunks)} processed ({len(summary[0]['summary_text'].split())} words)\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Chunk {i} produced empty summary\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚úï Chunk {i} failed: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # 5. FINAL SUMMARY GENERATION\n",
        "        if not successful_summaries:\n",
        "            raise RuntimeError(\"No chunks processed successfully\")\n",
        "\n",
        "        combined = ' '.join(successful_summaries)\n",
        "        final_word_count = len(combined.split())\n",
        "\n",
        "        # Adjust final summary length based on content\n",
        "        final_max = min(150, max(50, final_word_count // 2))\n",
        "        final_min = min(30, final_max // 2)\n",
        "\n",
        "        final_summary = summarizer(\n",
        "            combined,\n",
        "            max_length=final_max,\n",
        "            min_length=final_min,\n",
        "            truncation=True\n",
        "        )[0]['summary_text']\n",
        "\n",
        "        # 6. OUTPUT VALIDATION\n",
        "        if len(final_summary.split()) < 20:  # Fallback if too short\n",
        "            final_summary = successful_summaries[0] if successful_summaries else \"No meaningful summary generated\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_summary)\n",
        "\n",
        "        print(f\"\\n‚úî Success! Summary ({len(final_summary.split())} words) saved to {output_file}\")\n",
        "        return final_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üî• Critical failure: {str(e)}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "qCqr0gKmAE2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5 : Run Summarizer (transcription.txt ‚Üí call_summary.txt)"
      ],
      "metadata": {
        "id": "bFV0ecBXx4XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting advanced summarization...\")\n",
        "    summary = summarize_large_transcript(\"transcription.txt\")\n",
        "    if summary:\n",
        "        print(\"\\nüåà FINAL SUMMARY:\\n\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"‚ùå Summarization failed. Please check your input file.\")"
      ],
      "metadata": {
        "id": "fvze1bRSF3fB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}